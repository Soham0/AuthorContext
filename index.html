<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Authorcontext by iNeil77</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Authorcontext</h1>
      <h2 class="project-tagline">Information Retrieval and Extraction Project</h2>
      <a href="https://github.com/iNeil77/AuthorContext" class="btn">View on GitHub</a>
      <a href="https://github.com/iNeil77/AuthorContext/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/iNeil77/AuthorContext/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <hr>

<h2>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h2>

<p>This is a project maintained by myself, <strong>Indraneil Paul</strong> and co-contributor, <strong>Soham Saha</strong> that interprets a large number of Computer Science research papers from the DBLP archives and using the availble set of tags corresponding to each paper from an array of third-party sources, tries to predict a field in which a certain author is likely to contribute in the near-future.</p>

<p>Hosted here is a snapshot and brief discussion of the various challenges we faced and the ways that we tackled them.</p>

<hr>

<h2>
<a id="data-wrangling-and-mapping" class="anchor" href="#data-wrangling-and-mapping" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Data Wrangling and Mapping</h2>

<p>The original DBLP dump consisted of 294,000+ authors but their fields of work were not clearly mentioned. Using the previously done work of third party sources, tag information regarding the field of work of a large number of authors was found, andd then by matching the two sources we were able to salvage the tag information of as many as 216,000+ authors. A segment of the mapping has been shown below.</p>

<p align="center">
<img src="https://raw.githubusercontent.com/iNeil77/AuthorContext/gh-pages/IRE/2016-04-14_22-43-33.jpg">
<img src="https://raw.githubusercontent.com/iNeil77/AuthorContext/gh-pages/IRE/2016-04-14_22-48-41.jpg">
</p>

<p>The groundwork for the later learning, by negative sampling using a skip-gram model was also laid at this phase, with an author to authorID mapping created and the negative sampling tuples also written to a context file in the form <code>AuthorID1,AuthorID2,AuthorID3</code> where <code>AuthorID1</code> and <code>AuthorID2</code> could be two of the many authors who had collaborated on a paper. For each pair of authors, three different random negative samples were used in the form of three different <code>AuthorID3</code> which indicated three authors the collaborating pair had not co-collborated with. A segment of the context has been shown below.</p>

<p align="center">
<img src="https://raw.githubusercontent.com/iNeil77/AuthorContext/gh-pages/IRE/2016-04-14_22-48-41.jpg">
</p>

<hr>

<h2>
<a id="author2vec-and-negative-sampling" class="anchor" href="#author2vec-and-negative-sampling" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Author2Vec and Negative Sampling</h2>

<p>The next phase of our project invovlved the formulation of context vectors for each author using the skip-gram model as with the context file elaborated above, as input.</p>

<h4>
<a id="motivation-for-using-word-vectors" class="anchor" href="#motivation-for-using-word-vectors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Motivation for using word vectors</h4>

<p align="center">
<img src="https://raw.githubusercontent.com/iNeil77/AuthorContext/gh-pages/IRE/2016-04-14_15-45-13.jpg">
</p>

<p>Natural language processing systems traditionally treat words as discrete atomic symbols, and therefore 'cat' may be represented as <code>Id537</code> and 'dog' as <code>Id143</code>. These encodings are arbitrary, and provide no useful information to the system regarding the relationships that may exist between the individual symbols. This means that the model can leverage very little of what it has learned about 'cats' when it is processing data about 'dogs' (such that they are both animals, four-legged, pets, etc.). Representing words as unique, discrete ids furthermore leads to data sparsity, and usually means that we may need more data in order to successfully train statistical models. Using vector representations can overcome some of these obstacles. Vector space models (VSMs) represent (embed) words in a continuous vector space where semantically similar words are mapped to nearby points ('are embedded nearby each other').</p>

<p align="center">
<img src="https://raw.githubusercontent.com/iNeil77/AuthorContext/gh-pages/IRE/2016-04-14_15-29-33.jpg">
</p>

<p>Word2vec is a particularly computationally-efficient predictive model for learning word embeddings from raw text. It comes in two flavors, the Continuous Bag-of-Words model (CBOW) and the Skip-Gram model. Algorithmically, these models are similar, except that CBOW predicts target words (e.g. 'mat') from source context words ('the cat sits on the'), while the skip-gram does the inverse and predicts source context-words from the target words.</p>

<p align="center">
<img src="https://raw.githubusercontent.com/iNeil77/AuthorContext/gh-pages/IRE/word2vec_diagrams.png">
</p>

<p>This inversion might seem like an arbitrary choice, but statistically it has the effect that CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation). For the most part, this turns out to be a useful thing for smaller datasets. However, skip-gram treats each context-target pair as a new observation, and this tends to do better when we have larger datasets, which is why we went forward with it.</p>

<h4>
<a id="skip-gram-model" class="anchor" href="#skip-gram-model" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Skip-Gram Model</h4>

<p>Let's imagine at training step <code>t</code> we observe the first training case above, where the goal is to predict <code>the</code> from <code>quick</code>. We select <code>num_noise</code> number of noisy (contrastive) examples by drawing from some noise distribution, typically the unigram distribution, <code>P(w)</code>. For simplicity let's say <code>num_noise=1</code> and we select <code>sheep</code> as a noisy example. Next we compute the loss for this pair of observed and noisy examples. The goal is to make an update to the embedding parameters <code>θ</code> to improve (in this case, maximize) this objective function. </p>

<p align="center">
<img src="https://raw.githubusercontent.com/iNeil77/AuthorContext/gh-pages/IRE/2016-04-14_15-28-03.jpg">
</p>

<p>We do this by deriving the gradient of the loss with respect to the embedding parameters <code>θ</code>. We then perform an update to the embeddings by taking a small step in the direction of the gradient. </p>

<p align="center">
<img src="https://raw.githubusercontent.com/iNeil77/AuthorContext/gh-pages/IRE/Screenshot%20from%202016-04-15%2000-45-59.png">
</p>

<p>When this process is repeated over the entire training set, this has the effect of 'moving' the embedding vectors around for each word until the model is successful at discriminating real words from noise words.</p>

<p>Effectively, SKip Gram Negative Sampling is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant.</p>

<h4>
<a id="transfer-function" class="anchor" href="#transfer-function" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Transfer Function</h4>

<p>We took to the frequently-used <code>sigmoid</code> module as the trandfer function. This module applies the Sigmoid function element-wise to the input Tensor, thus outputting a Tensor of the same dimension. Sigmoid is defined as <code>f(x) = 1/(1+exp(-x))</code>.</p>

<h4>
<a id="loss-function-criteria" class="anchor" href="#loss-function-criteria" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Loss Function Criteria</h4>

<p>In this project we have used the Binary Cross Entropy Criterion which creates a criterion that measures the Binary Cross Entropy between the target and the output.</p>

<div class="highlight highlight-source-lua"><pre>criterion <span class="pl-k">=</span> nn.<span class="pl-c1">BCECriterion</span>([weights])</pre></div>

<p>This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets <code>t[i]</code> should be numbers between 0 and 1, for instance, the output of an <code>nn.Sigmoid</code> layer.</p>

<div class="highlight highlight-source-lua"><pre><span class="pl-c1">loss</span>(o, t) <span class="pl-k">=</span> <span class="pl-k">-</span> <span class="pl-c1">1</span><span class="pl-k">/</span>n <span class="pl-c1">sum_i</span> (t[i] <span class="pl-k">*</span> <span class="pl-c1">log</span>(o[i]) <span class="pl-k">+</span> (<span class="pl-c1">1</span> <span class="pl-k">-</span> t[i]) <span class="pl-k">*</span> <span class="pl-c1">log</span>(<span class="pl-c1">1</span> <span class="pl-k">-</span> o[i]))</pre></div>

<p>By default, the losses are averaged for each minibatch over observations as well as over dimensions. However, if the field <code>sizeAverage</code> is set to <code>false</code>, the losses are instead summed.</p>

<hr>

<h2>
<a id="transaction-speedup-and-efficiency" class="anchor" href="#transaction-speedup-and-efficiency" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Transaction Speedup and Efficiency</h2>

<p>The sheer size of the dataset was a challenge in itself. Runnning on subsets of over 40,000 authors proved to be infeasably slow and thus prompted us to look for other alternatives, as we needed to perform the task on a sample six times larger.</p>

<p>Also we needed a solution to the slew of memory-errors that we were frequently experiencing and as a result, we turned to the <code>lmdb.torch</code> plugin. This was essentially a wrapper for the <code>lightningmdb</code> plugin for <code>torch</code> which was an extremely efficient implementation of the memory-mapped database. The Lightning Memory-Mapped Database (LMDB) is a software library that provides a high-performance embedded transactional database in the form of a key-value store. LMDB is written in C with API bindings for several programming languages such as <code>torch</code> on <code>lua</code>. LMDB stores arbitrary key/data pairs as byte arrays, has a range-based search capability, supports multiple data items for a single key and has a special mode for appending records at the end of the database which gives a dramatic write performance increase over other similar stores.</p>

<p>Some attractive features of LMDB, that prompted us to use it in our project, include:</p>

<ul>
<li>   Its use of B+Tree. With an LMDB instance being in shared memory and the B+Tree block size being set to the OS page size, access to an LMDB store is extremely memory efficient.</li>
<li>   As LMDB is memory-mapped, it can return direct pointers to memory addresses of keys and values through its API, thereby avoiding unnecessary and expensive copying of memory. This results in greatly-increased performance (especially when the values stored are extremely large), and expands the potential use cases for LMDB.</li>
<li>   LMDB also tracks unused memory pages, using a B+Tree to keep track of pages freed (no longer needed) during transactions. By tracking unused pages the need for garbage-collection (and a garbage collection phase which would consume CPU cycles) is completely avoided.</li>
</ul>

<p>Also to the end of improving code efficiency, we used a lua library called <code>lds</code> which implements fast an efficient data structures. The <code>lds</code> library provides the containers <code>Array</code>, <code>Vector</code>, and <code>HashMap</code>, which cover the common use cases of Lua tables. However, the memory of these containers are managed through user-specified allocators (e.g. malloc) and thus are not subject to the LuaJIT memory limit. It is also significantly lower load on the LuaJIT Garbage Collector.</p>

<hr>

<h2>
<a id="classification" class="anchor" href="#classification" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Classification</h2>

<p>In the final phase an author was to be categorized into one of the fields in which he was most likely to work on in the near future. There were 25 such classes such as Artificial Intelligence, Computational Biology, etc which were assigned unique labels ranging from 0 to 24.</p>

<p>Subsequentlty, a technique of classification was to be chosen. Here we chose to use Support Vector Machine(SVM) to learn the classification boundaries, using the python library <code>scikit-learn</code>. Our choice of SVM as the technique of classification was influenced by the following factors:</p>

<ul>
<li>   Effective in high dimensional spaces. This was crucial to us since the embedding size of the vectors learnt for each author was a 100.</li>
<li>   Still effective in cases where number of dimensions is greater than the number of samples.</li>
<li>   Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient. Again, vital to us as we had an enormous archive dump to train and test upon.</li>
<li>   Versatile as different Kernel functions can be specified for the decision function. </li>
</ul>

<hr>

<h2>
<a id="results" class="anchor" href="#results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Results</h2>

<p>The <code>scikit-learn</code> package allows for two separate strategies to tackle multi-class classification problems using SVM, by providing the <code>SVC</code>, the <code>NuSVC</code> and the <code>Linear SVC</code> classes. Briefly discussed, they are:</p>

<ul>
<li>   The "One-vs-One Strategy": <code>SVC</code> and <code>NuSVC</code> implement the “one-against-one” approach (Knerr et al., 1990) for multi- class classification. If <code>n_class</code> is the number of classes, then <code>n_class * (n_class - 1) / 2</code> classifiers are constructed and each one trains data from two classes.</li>
<li>   The "One-vs-All Strategy": The <code>LinearSVC</code> implements “one-vs-the-rest” multi-class strategy, thus training <code>n_class</code> models. If there are only two classes, only one model is trained.</li>
<li>   Crammer and Singer Method: This is a multiclass SVM method which casts the multiclass classification problem into a single optimization problem, rather than decomposing it into multiple binary classification problems. The <code>LinearSVC</code> class implements this uding the <code>multi_class='crammer_singer'</code> option. This method is consistent, which is not true for one-vs-rest classification. In practice, one-vs-rest classification is usually preferred, since the results are mostly similar, but the runtime is significantly less.</li>
</ul>

<p align="center">
<img src="https://raw.githubusercontent.com/iNeil77/AuthorContext/gh-pages/IRE/2016-04-14_23-17-30.jpg">
</p>

<p>The <code>SVC</code> and <code>NuSVC</code> are similar methods, but accept slightly different sets of parameters and have different underlying mathematical formulations.  On the other hand, <code>LinearSVC</code> is another implementation of Support Vector Classification for the case of a linear kernel.</p>

<p>The results are obtained by firstlt training on 85 percent of the available data and then testing on the remaining 15 percent. This procedure is carried out for a group of 1000, 10000, 60000 and all the 216000+ unique authors, with the smaller subsets chosen randomly. The accuracy results obtained by using the above mentioned three strategies are tabulated below:</p>

<table>
<thead>
<tr>
<th align="right">Accuracies (in %)</th>
<th align="right">1,000</th>
<th align="right">10,000</th>
<th align="right">60,000</th>
<th align="right">Whole Dataset(216,000+)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">One-vs-one</td>
<td align="right">56</td>
<td align="right">61</td>
<td align="right">65</td>
<td align="right">68</td>
</tr>
<tr>
<td align="right">One-vs-All</td>
<td align="right">59</td>
<td align="right">63</td>
<td align="right">67</td>
<td align="right">69</td>
</tr>
<tr>
<td align="right">Crammer and Singer</td>
<td align="right">59</td>
<td align="right">64</td>
<td align="right">67</td>
<td align="right">72</td>
</tr>
</tbody>
</table>

<hr>

<h2>
<a id="references" class="anchor" href="#references" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>References</h2>

<p>The accompanying slides and reports can be found here at the project dropbox repository.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/iNeil77/AuthorContext">Authorcontext</a> is maintained by <a href="https://github.com/iNeil77">iNeil77</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
